/**
 * Tocin Standard Library - Machine Learning Module (Deep Learning)
 * Provides advanced deep learning components including transformers, attention mechanisms, and optimization techniques.
 */

import math.linear;
import math.stats;
import ml.neural_network;

/**
 * Deep learning optimizers for neural networks
 */
class Optimizer {
    /**
     * Base optimizer class
     */
    property learning_rate: float;
    property parameters: Array<Tensor>;
    property gradients: Array<Tensor>;
    
    def initialize(parameters: Array<Tensor>, learning_rate: float = 0.01) {
        self.parameters = parameters;
        self.learning_rate = learning_rate;
        self.gradients = [];
    }
    
    def step() {
        throw NotImplementedError("Optimizer.step must be implemented by subclass");
    }
    
    def zero_grad() {
        for (let i = 0; i < self.parameters.length; i++) {
            self.gradients[i] = Tensor.zeros_like(self.parameters[i]);
        }
    }

    /**
     * SGD optimizer with momentum
     */
    class SGD extends Optimizer {
        property momentum: float;
        property velocity: Array<Tensor>;
        
        def initialize(parameters: Array<Tensor>, learning_rate: float = 0.01, momentum: float = 0.9) {
            super.initialize(parameters, learning_rate);
            self.momentum = momentum;
            self.velocity = [];
            
            for (let param in parameters) {
                self.velocity.push(Tensor.zeros_like(param));
            }
        }
        
        def step() {
            for (let i = 0; i < self.parameters.length; i++) {
                self.velocity[i] = self.velocity[i].scale(self.momentum) + self.gradients[i].scale(1.0);
                self.parameters[i] = self.parameters[i].subtract(self.velocity[i].scale(self.learning_rate));
            }
        }
    }
    
    /**
     * Adam optimizer
     */
    class Adam extends Optimizer {
        property beta1: float;
        property beta2: float;
        property epsilon: float;
        property m: Array<Tensor>;
        property v: Array<Tensor>;
        property t: int;
        
        def initialize(parameters: Array<Tensor>, learning_rate: float = 0.001, 
                      beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8) {
            super.initialize(parameters, learning_rate);
            self.beta1 = beta1;
            self.beta2 = beta2;
            self.epsilon = epsilon;
            self.t = 0;
            self.m = [];
            self.v = [];
            
            for (let param in parameters) {
                self.m.push(Tensor.zeros_like(param));
                self.v.push(Tensor.zeros_like(param));
            }
        }
        
        def step() {
            self.t += 1;
            let beta1_t = math.pow(self.beta1, self.t);
            let beta2_t = math.pow(self.beta2, self.t);
            
            for (let i = 0; i < self.parameters.length; i++) {
                // Update biased first moment estimate
                self.m[i] = self.m[i].scale(self.beta1).add(self.gradients[i].scale(1 - self.beta1));
                
                // Update biased second raw moment estimate
                self.v[i] = self.v[i].scale(self.beta2).add(
                    self.gradients[i].multiply(self.gradients[i]).scale(1 - self.beta2)
                );
                
                // Bias correction
                let m_hat = self.m[i].scale(1.0 / (1.0 - beta1_t));
                let v_hat = self.v[i].scale(1.0 / (1.0 - beta2_t));
                
                // Update parameters
                self.parameters[i] = self.parameters[i].subtract(
                    m_hat.divide(v_hat.sqrt().add(self.epsilon)).scale(self.learning_rate)
                );
            }
        }
    }
}

/**
 * Attention mechanism for sequence models
 */
class Attention {
    property d_model: int;
    property weights: {query: Tensor, key: Tensor, value: Tensor, output: Tensor};
    
    def initialize(d_model: int) {
        self.d_model = d_model;
        
        // Initialize weights
        self.weights = {
            query: Tensor.randn([d_model, d_model]).scale(0.1),
            key: Tensor.randn([d_model, d_model]).scale(0.1),
            value: Tensor.randn([d_model, d_model]).scale(0.1),
            output: Tensor.randn([d_model, d_model]).scale(0.1)
        };
    }
    
    def forward(query: Tensor, key: Tensor, value: Tensor, mask: Tensor? = null) -> {output: Tensor, attention: Tensor} {
        let batch_size = query.shape[0];
        
        // Linear projections
        let Q = query.matmul(self.weights.query);
        let K = key.matmul(self.weights.key);
        let V = value.matmul(self.weights.value);
        
        // Scaled dot-product attention
        let scores = Q.matmul(K.transpose(0, 2, 1));
        scores = scores.scale(1.0 / math.sqrt(self.d_model));
        
        // Apply mask if provided
        if (mask != null) {
            scores = scores.maskedFill(mask, -1e9);
        }
        
        // Softmax to get attention weights
        let attention_weights = scores.softmax(-1);
        
        // Apply attention weights to values
        let context = attention_weights.matmul(V);
        
        // Final linear projection
        let output = context.matmul(self.weights.output);
        
        return {
            output: output,
            attention: attention_weights
        };
    }
}

/**
 * Multi-head attention for transformer models
 */
class MultiHeadAttention {
    property d_model: int;
    property num_heads: int;
    property d_k: int;
    property attention_heads: Array<Attention>;
    property output_linear: Tensor;
    
    def initialize(d_model: int, num_heads: int) {
        self.d_model = d_model;
        self.num_heads = num_heads;
        self.d_k = d_model / num_heads;
        
        if (d_model % num_heads != 0) {
            throw ValueError("d_model must be divisible by num_heads");
        }
        
        // Initialize attention heads
        self.attention_heads = [];
        for (let i = 0; i < num_heads; i++) {
            self.attention_heads.push(Attention(self.d_k));
        }
        
        // Output linear layer
        self.output_linear = Tensor.randn([d_model, d_model]).scale(0.1);
    }
    
    def forward(query: Tensor, key: Tensor, value: Tensor, mask: Tensor? = null) -> {output: Tensor, attention: Tensor} {
        let batch_size = query.shape[0];
        
        // Split input for multi-head attention
        let query_heads = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2);
        let key_heads = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2);
        let value_heads = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2);
        
        // Apply attention for each head
        let head_outputs = [];
        let attentions = [];
        
        for (let i = 0; i < self.num_heads; i++) {
            let head_result = self.attention_heads[i].forward(
                query_heads[:, i],
                key_heads[:, i],
                value_heads[:, i],
                mask
            );
            head_outputs.push(head_result.output);
            attentions.push(head_result.attention);
        }
        
        // Concatenate and reshape head outputs
        let output = Tensor.cat(head_outputs, 2);
        output = output.view(batch_size, -1, self.d_model);
        
        // Final linear projection
        output = output.matmul(self.output_linear);
        
        return {
            output: output,
            attention: Tensor.stack(attentions, 1)
        };
    }
}

/**
 * Position-wise Feed-Forward Network for transformers
 */
class PositionwiseFeedForward {
    property d_model: int;
    property d_ff: int;
    property weights1: Tensor;
    property weights2: Tensor;
    property bias1: Tensor;
    property bias2: Tensor;
    
    def initialize(d_model: int, d_ff: int) {
        self.d_model = d_model;
        self.d_ff = d_ff;
        
        // Initialize weights
        self.weights1 = Tensor.randn([d_model, d_ff]).scale(0.1);
        self.weights2 = Tensor.randn([d_ff, d_model]).scale(0.1);
        self.bias1 = Tensor.zeros([d_ff]);
        self.bias2 = Tensor.zeros([d_model]);
    }
    
    def forward(x: Tensor) -> Tensor {
        // First linear layer with ReLU
        let output = x.matmul(self.weights1).add(self.bias1);
        output = output.relu();
        
        // Second linear layer
        output = output.matmul(self.weights2).add(self.bias2);
        
        return output;
    }
}

/**
 * Positional Encoding for transformers
 */
class PositionalEncoding {
    property d_model: int;
    property max_seq_length: int;
    property encoding: Tensor;
    property dropout: float;
    
    def initialize(d_model: int, max_seq_length: int = 5000, dropout: float = 0.1) {
        self.d_model = d_model;
        self.max_seq_length = max_seq_length;
        self.dropout = dropout;
        
        // Create positional encoding
        self.encoding = Tensor.zeros([max_seq_length, d_model]);
        
        // Calculate positional encoding
        let position = Tensor.arange(0, max_seq_length).reshape(-1, 1);
        let div_term = Tensor.exp(Tensor.arange(0, d_model, 2).scale(-math.log(10000.0) / d_model));
        
        self.encoding[:, 0::2] = Tensor.sin(position.matmul(div_term.transpose(0, 1)));
        self.encoding[:, 1::2] = Tensor.cos(position.matmul(div_term.transpose(0, 1)));
        
        // Add batch dimension
        self.encoding = self.encoding.unsqueeze(0);
    }
    
    def forward(x: Tensor) -> Tensor {
        // x shape: [batch_size, seq_length, d_model]
        let seq_length = x.shape[1];
        
        // Add positional encoding
        let output = x + self.encoding[:, :seq_length, :];
        
        // Apply dropout
        output = output.dropout(self.dropout);
        
        return output;
    }
}

/**
 * Encoder layer for transformer models
 */
class TransformerEncoderLayer {
    property self_attn: MultiHeadAttention;
    property feed_forward: PositionwiseFeedForward;
    property layer_norm1: LayerNorm;
    property layer_norm2: LayerNorm;
    property dropout: float;
    
    def initialize(d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1) {
        self.self_attn = MultiHeadAttention(d_model, num_heads);
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff);
        self.layer_norm1 = LayerNorm(d_model);
        self.layer_norm2 = LayerNorm(d_model);
        self.dropout = dropout;
    }
    
    def forward(x: Tensor, mask: Tensor? = null) -> Tensor {
        // Self attention with skip connection and layer norm
        let attn_output = self.self_attn.forward(x, x, x, mask).output;
        let out1 = self.layer_norm1.forward(x + attn_output.dropout(self.dropout));
        
        // Feed forward with skip connection and layer norm
        let ff_output = self.feed_forward.forward(out1);
        let out2 = self.layer_norm2.forward(out1 + ff_output.dropout(self.dropout));
        
        return out2;
    }
}

/**
 * Decoder layer for transformer models
 */
class TransformerDecoderLayer {
    property self_attn: MultiHeadAttention;
    property cross_attn: MultiHeadAttention;
    property feed_forward: PositionwiseFeedForward;
    property layer_norm1: LayerNorm;
    property layer_norm2: LayerNorm;
    property layer_norm3: LayerNorm;
    property dropout: float;
    
    def initialize(d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1) {
        self.self_attn = MultiHeadAttention(d_model, num_heads);
        self.cross_attn = MultiHeadAttention(d_model, num_heads);
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff);
        self.layer_norm1 = LayerNorm(d_model);
        self.layer_norm2 = LayerNorm(d_model);
        self.layer_norm3 = LayerNorm(d_model);
        self.dropout = dropout;
    }
    
    def forward(x: Tensor, memory: Tensor, src_mask: Tensor? = null, tgt_mask: Tensor? = null) -> Tensor {
        // Self attention with skip connection and layer norm
        let self_attn_output = self.self_attn.forward(x, x, x, tgt_mask).output;
        let out1 = self.layer_norm1.forward(x + self_attn_output.dropout(self.dropout));
        
        // Cross attention with skip connection and layer norm
        let cross_attn_output = self.cross_attn.forward(out1, memory, memory, src_mask).output;
        let out2 = self.layer_norm2.forward(out1 + cross_attn_output.dropout(self.dropout));
        
        // Feed forward with skip connection and layer norm
        let ff_output = self.feed_forward.forward(out2);
        let out3 = self.layer_norm3.forward(out2 + ff_output.dropout(self.dropout));
        
        return out3;
    }
}

/**
 * Layer normalization for transformer models
 */
class LayerNorm {
    property d_model: int;
    property gamma: Tensor;
    property beta: Tensor;
    property epsilon: float;
    
    def initialize(d_model: int, epsilon: float = 1e-5) {
        self.d_model = d_model;
        self.gamma = Tensor.ones([d_model]);
        self.beta = Tensor.zeros([d_model]);
        self.epsilon = epsilon;
    }
    
    def forward(x: Tensor) -> Tensor {
        // Calculate mean and variance
        let mean = x.mean(-1, keepdim=true);
        let var = x.var(-1, unbiased=false, keepdim=true);
        
        // Normalize
        let output = (x - mean) / (var + self.epsilon).sqrt();
        
        // Scale and shift
        output = output * self.gamma + self.beta;
        
        return output;
    }
}

/**
 * Complete Transformer model
 */
class Transformer {
    property encoder_layers: Array<TransformerEncoderLayer>;
    property decoder_layers: Array<TransformerDecoderLayer>;
    property src_embed: Embedding;
    property tgt_embed: Embedding;
    property positional_encoding: PositionalEncoding;
    property output_linear: Tensor;
    property d_model: int;
    property vocab_size: int;
    
    def initialize(vocab_size: int, d_model: int, num_heads: int, num_layers: int, d_ff: int, 
                  max_seq_length: int = 5000, dropout: float = 0.1) {
        self.d_model = d_model;
        self.vocab_size = vocab_size;
        
        // Embeddings
        self.src_embed = Embedding(vocab_size, d_model);
        self.tgt_embed = Embedding(vocab_size, d_model);
        
        // Positional encoding
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout);
        
        // Encoder layers
        self.encoder_layers = [];
        for (let i = 0; i < num_layers; i++) {
            self.encoder_layers.push(TransformerEncoderLayer(d_model, num_heads, d_ff, dropout));
        }
        
        // Decoder layers
        self.decoder_layers = [];
        for (let i = 0; i < num_layers; i++) {
            self.decoder_layers.push(TransformerDecoderLayer(d_model, num_heads, d_ff, dropout));
        }
        
        // Output projection
        self.output_linear = Tensor.randn([d_model, vocab_size]).scale(0.1);
    }
    
    def encode(src: Tensor, src_mask: Tensor? = null) -> Tensor {
        // Embed source tokens
        let src_embedded = self.src_embed.forward(src).scale(math.sqrt(self.d_model));
        
        // Add positional encoding
        let src_encoded = self.positional_encoding.forward(src_embedded);
        
        // Pass through encoder layers
        let encoder_output = src_encoded;
        for (let layer of self.encoder_layers) {
            encoder_output = layer.forward(encoder_output, src_mask);
        }
        
        return encoder_output;
    }
    
    def decode(tgt: Tensor, memory: Tensor, src_mask: Tensor? = null, tgt_mask: Tensor? = null) -> Tensor {
        // Embed target tokens
        let tgt_embedded = self.tgt_embed.forward(tgt).scale(math.sqrt(self.d_model));
        
        // Add positional encoding
        let tgt_encoded = self.positional_encoding.forward(tgt_embedded);
        
        // Pass through decoder layers
        let decoder_output = tgt_encoded;
        for (let layer of self.decoder_layers) {
            decoder_output = layer.forward(decoder_output, memory, src_mask, tgt_mask);
        }
        
        return decoder_output;
    }
    
    def forward(src: Tensor, tgt: Tensor, src_mask: Tensor? = null, tgt_mask: Tensor? = null) -> Tensor {
        // Encode source
        let memory = self.encode(src, src_mask);
        
        // Decode target
        let output = self.decode(tgt, memory, src_mask, tgt_mask);
        
        // Project to vocabulary
        output = output.matmul(self.output_linear);
        
        return output;
    }
    
    def generate(src: Tensor, src_mask: Tensor? = null, max_length: int = 100, 
                eos_token: int = 2, device: string = "cpu") -> Tensor {
        let batch_size = src.shape[0];
        
        // Encode source
        let memory = self.encode(src, src_mask);
        
        // Start with BOS token (assuming token 1)
        let ys = Tensor.ones([batch_size, 1]).cast(int);
        
        for (let i = 0; i < max_length - 1; i++) {
            // Generate target mask (to prevent looking ahead)
            let tgt_mask = self.generate_square_subsequent_mask(ys.shape[1], device);
            
            // Decode
            let out = self.decode(ys, memory, src_mask, tgt_mask);
            
            // Get next token probabilities
            let prob = out[:, -1].matmul(self.output_linear).softmax(-1);
            
            // Sample from distribution or take argmax
            let next_word = prob.argmax(-1).unsqueeze(1);
            
            // Concatenate with output sequence
            ys = Tensor.cat([ys, next_word], 1);
            
            // Check if all sequences have EOS
            if (ys.eq(eos_token).any(1).all()) {
                break;
            }
        }
        
        return ys;
    }
    
    static def generate_square_subsequent_mask(size: int, device: string = "cpu") -> Tensor {
        // Create a square mask for decoder self-attention
        // The mask ensures that position i cannot attend to positions j > i
        
        mask = Tensor.ones([size, size], device=device).triu(1);
        return mask.eq(1);
    }
}

/**
 * Embedding layer for transformer models
 */
class Embedding {
    property num_embeddings: int;
    property embedding_dim: int;
    property weight: Tensor;
    
    def initialize(num_embeddings: int, embedding_dim: int) {
        self.num_embeddings = num_embeddings;
        self.embedding_dim = embedding_dim;
        
        // Initialize embedding weights
        self.weight = Tensor.randn([num_embeddings, embedding_dim]).scale(0.1);
    }
    
    def forward(input: Tensor) -> Tensor {
        // Input shape: [batch_size, seq_length]
        // Output shape: [batch_size, seq_length, embedding_dim]
        
        return input.embedding(self.weight);
    }
}

/**
 * BERT model implementation
 */
class BERT {
    property transformer: Transformer;
    property token_type_embed: Embedding;
    property pooler: {weight: Tensor, bias: Tensor};
    property d_model: int;
    property vocab_size: int;
    
    def initialize(vocab_size: int, d_model: int = 768, num_heads: int = 12, num_layers: int = 12, 
                  d_ff: int = 3072, max_seq_length: int = 512, dropout: float = 0.1) {
        self.d_model = d_model;
        self.vocab_size = vocab_size;
        
        // Initialize transformer
        self.transformer = Transformer(
            vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout
        );
        
        // Token type embeddings (for segment embeddings in BERT)
        self.token_type_embed = Embedding(2, d_model);
        
        // Pooler for [CLS] token representation
        self.pooler = {
            weight: Tensor.randn([d_model, d_model]).scale(0.1),
            bias: Tensor.zeros([d_model])
        };
    }
    
    def forward(input_ids: Tensor, token_type_ids: Tensor? = null, attention_mask: Tensor? = null) -> {
        sequence_output: Tensor,
        pooled_output: Tensor
    } {
        // Default token_type_ids
        if (token_type_ids == null) {
            token_type_ids = Tensor.zeros_like(input_ids);
        }
        
        // Convert attention mask to causal mask for transformer
        let causal_mask = null;
        if (attention_mask != null) {
            causal_mask = attention_mask.unsqueeze(1).unsqueeze(2);
            causal_mask = (1.0 - causal_mask) * -10000.0;
        }
        
        // Get embeddings
        let inputs_embeds = self.transformer.src_embed.forward(input_ids);
        let token_type_embeds = self.token_type_embed.forward(token_type_ids);
        
        // Combine embeddings
        let embeddings = inputs_embeds + token_type_embeds;
        
        // Add positional encoding
        let hidden_states = self.transformer.positional_encoding.forward(embeddings);
        
        // Pass through encoder
        let sequence_output = hidden_states;
        for (let layer of self.transformer.encoder_layers) {
            sequence_output = layer.forward(sequence_output, causal_mask);
        }
        
        // Get pooled output from [CLS] token (first token)
        let first_token = sequence_output[:, 0];
        let pooled_output = Tensor.tanh(first_token.matmul(self.pooler.weight) + self.pooler.bias);
        
        return {
            sequence_output,
            pooled_output
        };
    }
}

/**
 * GPT model implementation
 */
class GPT {
    property transformer: Transformer;
    property d_model: int;
    property vocab_size: int;
    
    def initialize(vocab_size: int, d_model: int = 768, num_heads: int = 12, num_layers: int = 12, 
                  d_ff: int = 3072, max_seq_length: int = 1024, dropout: float = 0.1) {
        self.d_model = d_model;
        self.vocab_size = vocab_size;
        
        // Initialize transformer
        self.transformer = Transformer(
            vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout
        );
    }
    
    def forward(input_ids: Tensor, past_key_values: Array<Tensor>? = null, 
               attention_mask: Tensor? = null) -> {
        logits: Tensor,
        past_key_values: Array<Tensor>?
    } {
        // Prepare causal mask
        let seq_length = input_ids.shape[1];
        let causal_mask = Transformer.generate_square_subsequent_mask(seq_length);
        
        if (attention_mask != null) {
            // Combine with provided attention mask
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2);
            causal_mask = causal_mask.logical_or(attention_mask.eq(0));
        }
        
        // Embed inputs
        let inputs_embeds = self.transformer.src_embed.forward(input_ids);
        
        // Add positional encoding
        let hidden_states = self.transformer.positional_encoding.forward(inputs_embeds);
        
        // Use past key values if provided (for faster generation)
        let new_past_key_values = [];
        
        // Pass through decoder layers (using encoder layers as autoregressive layers)
        for (let i = 0; i < self.transformer.encoder_layers.length; i++) {
            let layer = self.transformer.encoder_layers[i];
            
            if (past_key_values != null) {
                // Use cached key/values (not fully implemented in this example)
                // This would require modifying the attention layers to accept past_key_values
            }
            
            hidden_states = layer.forward(hidden_states, causal_mask);
            
            // Cache key/values for next step (not fully implemented in this example)
            // new_past_key_values.push(...);
        }
        
        // Project to vocabulary
        let logits = hidden_states.matmul(self.transformer.output_linear);
        
        return {
            logits,
            past_key_values: new_past_key_values.length > 0 ? new_past_key_values : null
        };
    }
    
    def generate(input_ids: Tensor, max_length: int = 100, temperature: float = 1.0, 
                top_k: int? = null, top_p: float? = null, repetition_penalty: float = 1.0, 
                eos_token_id: int? = null) -> Tensor {
        let batch_size = input_ids.shape[0];
        let seq_length = input_ids.shape[1];
        
        // Make a copy of the input
        let generated = input_ids.clone();
        
        // Track which sequences are finished
        let is_finished = Tensor.zeros([batch_size]).cast(bool);
        
        let past_key_values = null;
        
        for (let i = 0; i < max_length - seq_length; i++) {
            // Get only the last token
            let current_input = generated[:, -1].unsqueeze(1);
            
            // Forward pass
            let outputs = self.forward(current_input, past_key_values);
            let next_token_logits = outputs.logits[:, -1, :];
            past_key_values = outputs.past_key_values;
            
            // Apply temperature
            next_token_logits = next_token_logits.scale(1.0 / temperature);
            
            // Apply repetition penalty
            if (repetition_penalty != 1.0) {
                // Get indices of tokens that have already been generated
                for (let b = 0; b < batch_size; b++) {
                    let prev_tokens = generated[b];
                    // Update logits for previously generated tokens
                    for (let token of prev_tokens) {
                        if (next_token_logits[b, token] > 0) {
                            next_token_logits[b, token] /= repetition_penalty;
                        } else {
                            next_token_logits[b, token] *= repetition_penalty;
                        }
                    }
                }
            }
            
            // Apply top-k sampling
            if (top_k != null && top_k > 0) {
                next_token_logits = self._top_k_filtering(next_token_logits, top_k);
            }
            
            // Apply top-p (nucleus) sampling
            if (top_p != null && top_p < 1.0) {
                next_token_logits = self._top_p_filtering(next_token_logits, top_p);
            }
            
            // Get probabilities
            let probs = next_token_logits.softmax(-1);
            
            // Sample from the distribution
            let next_tokens = probs.multinomial(1);
            
            // Mark sequences that have reached the EOS token as finished
            if (eos_token_id != null) {
                is_finished = is_finished.logical_or(next_tokens.eq(eos_token_id).squeeze(1));
            }
            
            // Add new tokens to the sequence
            generated = Tensor.cat([generated, next_tokens], 1);
            
            // Stop if all sequences are finished
            if (is_finished.all()) {
                break;
            }
        }
        
        return generated;
    }
    
    private def _top_k_filtering(logits: Tensor, k: int) -> Tensor {
        // Keep only the top-k tokens
        let values, indices = logits.topk(k, 1);
        let filtered_logits = Tensor.full_like(logits, -float("Inf"));
        
        for (let i = 0; i < logits.shape[0]; i++) {
            filtered_logits[i].index_put_(indices[i], values[i]);
        }
        
        return filtered_logits;
    }
    
    private def _top_p_filtering(logits: Tensor, p: float) -> Tensor {
        // Sort logits in descending order
        let sorted_logits, sorted_indices = logits.sort(1, descending=true);
        
        // Calculate cumulative probabilities
        let cumulative_probs = sorted_logits.softmax(-1).cumsum(1);
        
        // Create mask for tokens to remove
        let mask = cumulative_probs < p;
        
        // Add a column of 1s to ensure at least one token is kept
        mask[:, 0] = true;
        
        // Apply mask to get filtered indices
        let filtered_indices = [];
        for (let i = 0; i < logits.shape[0]; i++) {
            filtered_indices.push(sorted_indices[i][mask[i]]);
        }
        
        // Create filtered logits
        let filtered_logits = Tensor.full_like(logits, -float("Inf"));
        
        for (let i = 0; i < logits.shape[0]; i++) {
            filtered_logits[i].index_put_(filtered_indices[i], logits[i].index_select(0, filtered_indices[i]));
        }
        
        return filtered_logits;
    }
} 
